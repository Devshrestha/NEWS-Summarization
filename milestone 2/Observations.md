# Observations

##  Dataset
Dataset is too large for our memory collab and local machine<br>
Should imrove if pipelining is used <br>
This data set contains articles from international NEWs might have to fine tune for local NEWS

## Models
For seq2seq training is very slow. <br>
The predictions are not that good, the model fails to preserve contextual meaning between words.

## Improvements 
Improve by using embeddings word2vec or Elmo <br>
Can improve by using attention <br>
More complex models:<br>
Transformers,BERT
